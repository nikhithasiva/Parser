{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7b72fe-fa1d-4e00-b258-a779e3e2a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.0-cp39-abi3-macosx_11_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0dd732c-552b-4b92-a14a-660dd910b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl (42 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl (126 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.7/634.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl (839 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.4/839.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (174 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 numpy-2.2.6 preshed-3.0.10 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9203871-0b23-476c-bf49-4a9f7f3d3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Extracted 4948 sentences with context\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting fields: 100%|██████████████████████████| 9/9 [00:07<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED EXTRACTION RESULTS\n",
      "============================================================\n",
      "{\n",
      "  \"fund_name\": \"BlackRock Private Credit Fund\",\n",
      "  \"sponsor\": \"and BlackRock Advisors, LLC\",\n",
      "  \"inception_year\": \"2022\",\n",
      "  \"annualized_distribution_rate\": \"5.0%\",\n",
      "  \"total_investments_fair_value\": \"$400.9 million\",\n",
      "  \"management_fee\": \"2.0% + 2.0% + 2.0% + 1.25% + 1.25% + 1.25% + 0.85% + 0.25% + 0.00% + 9.62% + 9.62% + 9.62% + 0.80% + 0.80% + 0.80% + 3.5% + 1.5%\",\n",
      "  \"asset_allocation\": \"Level 1: 36.2%; Level 2: 36.2%; Level 3: 63.8%\",\n",
      "  \"suitability_requirements\": \"$398,929,289 income and $210,903,951, net worth\",\n",
      "  \"liquidity\": \"Share Repurchase Program\\nAt the discretion of the Fund’s Board of Trustees, the Fund is conducting a share repurchase program in which the Fund is repurchasing, in each quarter, up to 5% of the Fund’s...\",\n",
      "  \"assumptions_made\": [\n",
      "    \"suitability extracted with medium confidence (score: 7.3)\",\n",
      "    \"liquidity - using best context match (score: 12.2)\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Download spaCy model if not already present\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "\n",
    "@dataclass\n",
    "class FundInfo:\n",
    "    fund_name: str = \"\"\n",
    "    sponsor: str = \"\"\n",
    "    inception_year: str = \"\"\n",
    "    annualized_distribution_rate: str = \"\"\n",
    "    total_investments_fair_value: str = \"\"\n",
    "    management_fee: str = \"\"\n",
    "    asset_allocation: str = \"\"\n",
    "    suitability_requirements: str = \"\"\n",
    "    liquidity: str = \"\"\n",
    "    assumptions_made: List[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.assumptions_made is None:\n",
    "            self.assumptions_made = []\n",
    "\n",
    "class EnhancedFundParser:\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        \n",
    "        # Set up device\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Extract structured content\n",
    "        self.pages_content = self._extract_pages_with_structure()\n",
    "        self.text = \"\\n\".join([page[\"text\"] for page in self.pages_content])\n",
    "        \n",
    "        # Initialize spaCy\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.nlp.max_length = 2000000\n",
    "        \n",
    "        # Extract sentences with context\n",
    "        self.sentences_with_context = self._extract_sentences_with_context()\n",
    "\n",
    "        # Initialize BERT model\n",
    "        print(\"Loading BERT model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Enhanced field configurations with better patterns and section wise hints\n",
    "        self.field_contexts = {\n",
    "            \"fund_name\": {\n",
    "                \"context_keywords\": [\n",
    "                    \"fund name\", \"private credit fund\", \"blackrock\", \"investment company\", \n",
    "                    \"prospectus\", \"fund\", \"blackrock private credit\", \"series\", \"class\",\n",
    "                    \"institutional shares\", \"investor shares\", \"portfolio\", \"the fund\",\n",
    "                    \"closed-end fund\", \"interval fund\", \"credit fund\", \"name of fund\",\n",
    "                    \"registrant\", \"fund title\"\n",
    "                ],\n",
    "                \"section_hints\": [\n",
    "                    \"cover\", \"summary\", \"prospectus summary\", \"fund overview\", \"title\",\n",
    "                    \"front matter\", \"header\", \"fund information\", \"general information\",\n",
    "                    \"investment objective\", \"fund details\"\n",
    "                ],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"BlackRock\\s+Private\\s+Credit\\s+Fund\",\n",
    "                    r\"BlackRock\\s+[A-Za-z\\s&]*(?:Private\\s+)?Credit\\s+Fund\",\n",
    "                    r\"(?:The\\s+)?BlackRock\\s+Private\\s+Credit\\s+Fund\",\n",
    "                    r\"Fund\\s+Name[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                    r\"(BlackRock[A-Za-z\\s&]*Fund)(?:\\s+\\(|,|\\.|$)\",\n",
    "                    r\"Registrant[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                    r\"Investment\\s+Company[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                    r\"Series[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                    r\"(?:^|\\n)([A-Za-z\\s]*BlackRock[A-Za-z\\s]*Private[A-Za-z\\s]*Credit[A-Za-z\\s]*Fund)\",\n",
    "                    r\"([A-Za-z\\s]*Private\\s+Credit\\s+Fund)\",\n",
    "                    r\"BlackRock\\s+([A-Za-z\\s&]+(?:Fund|Trust|Income|Credit))\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"fee\", \"expense\", \"manager\", \"advisor\", \"client\", \"employee\"],\n",
    "                \"expected_format\": \"company_name\"\n",
    "            },\n",
    "            \"sponsor\": {\n",
    "                \"context_keywords\": [\n",
    "                    \"investment adviser\", \"sponsor\", \"adviser\", \"advisor\", \"fund manager\", \n",
    "                    \"managed by\", \"blackrock\", \"investment advisor\", \"portfolio manager\",\n",
    "                    \"fund adviser\", \"general partner\", \"management company\", \"sub-adviser\",\n",
    "                    \"investment management\", \"asset manager\", \"fund sponsor\", \"administers\",\n",
    "                    \"investment advisory\", \"serves as\", \"manager\", \"advises\", \"principal\",\n",
    "                    \"blackrock advisors\", \"blackrock inc\", \"blackrock fund advisors\"\n",
    "                ],\n",
    "                \"section_hints\": [\n",
    "                    \"management\", \"sponsor\", \"advisor\", \"investment adviser\", \"fund management\",\n",
    "                    \"portfolio management\", \"advisory services\", \"fund advisor\", \"management company\",\n",
    "                    \"investment advisory\", \"administrator\", \"general information\"\n",
    "                ],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"Investment\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Advisors?|Management)?)\",\n",
    "                    r\"Sponsor[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC)?)\",\n",
    "                    r\"(BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Advisors?|Management|Fund\\s+Advisors?))\",\n",
    "                    r\"managed\\s+by\\s+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Advisors?))\",\n",
    "                    r\"Investment\\s+Manager[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*)\",\n",
    "                    r\"Portfolio\\s+Manager[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*)\",\n",
    "                    r\"Fund\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*)\",\n",
    "                    r\"serves\\s+as.*?(?:adviser?|manager)[:\\s]*([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*)\",\n",
    "                    r\"(BlackRock)(?:\\s+Inc\\.?|\\s+LLC|\\s+Advisors?)?(?:\\s+serves\\s+as|\\s+acts\\s+as|\\s+is\\s+the)\",\n",
    "                    r\"(?:^|\\n|\\s)(BlackRock)(?:\\s+Inc\\.?|\\s+Fund\\s+Advisors?|\\s+Advisors?)?(?=\\s+is\\s+the|\\s+serves|\\s+acts)\",\n",
    "                    r\"Investment\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]+(?:Inc|LLC|LP|Advisers?))\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"client\", \"investor\", \"shareholder\", \"distribution\", \"employee\", \"board\"],\n",
    "                \"expected_format\": \"company_name\"\n",
    "            },\n",
    "            \"inception_year\": {\n",
    "                \"context_keywords\": [\"inception\", \"commenced operations\", \"fund inception\", \"launched\", \"formed\", \"established\"],\n",
    "                \"section_hints\": [\"fund history\", \"inception\", \"operations\", \"summary\", \"overview\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"(?:inception|commenced|launched|formed|established).{0,50}(20\\d{2})\",\n",
    "                    r\"(20\\d{2}).{0,50}(?:inception|commenced|launched)\",\n",
    "                    r\"Fund\\s+inception[:\\s]+(20\\d{2})\",\n",
    "                    r\"since\\s+(20\\d{2})\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"expiration\", \"maturity\", \"termination\"],\n",
    "                \"expected_format\": \"year\"\n",
    "            },\n",
    "            \"distribution_rate\": {\n",
    "                \"context_keywords\": [\"distribution rate\", \"target distribution\", \"annualized\", \"yield\", \"effective yield\", \"monthly distribution\"],\n",
    "                \"section_hints\": [\"distributions\", \"dividend policy\", \"yield\", \"performance\", \"summary\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"(\\d+\\.\\d+%)\\s*(?:annualized|effective|target|distribution)\",\n",
    "                    r\"(?:annualized|effective|target)\\s+(?:distribution\\s+rate|yield)[:\\s]*(\\d+\\.\\d+%)\",\n",
    "                    r\"\\$\\d+\\.\\d+\\s*(?:per\\s+share|\\/share).*?(?:monthly|quarterly)\",\n",
    "                    r\"target.*?(\\d+\\.\\d+%)\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"expense\", \"fee\", \"cost\", \"management\"],\n",
    "                \"expected_format\": \"percentage_or_dollar\"\n",
    "            },\n",
    "            \"total_investments\": {\n",
    "                \"context_keywords\": [\"total investments\", \"fair value\", \"portfolio value\", \"net assets\", \"total assets\"],\n",
    "                \"section_hints\": [\"portfolio\", \"investments\", \"financial statements\", \"balance sheet\", \"assets\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"[Tt]otal\\s+investments[:\\s]*\\$([0-9,]+(?:\\.[0-9]+)?)\\s*(?:million|billion)?\",\n",
    "                    r\"\\$([0-9,]+(?:\\.[0-9]+)?)\\s*(?:million|billion)?.{0,30}(?:total\\s+investments|fair\\s+value)\",\n",
    "                    r\"(?:Total|Net)\\s+(?:investments|assets)[:\\s]*\\$([0-9,]+(?:\\.[0-9]+)?)\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"fee\", \"expense\", \"liability\", \"distribution\"],\n",
    "                \"expected_format\": \"dollar_amount\"\n",
    "            },\n",
    "            \"management_fee\": {\n",
    "                \"context_keywords\": [\"management fee\", \"base fee\", \"advisory fee\", \"expense ratio\", \"fees\"],\n",
    "                \"section_hints\": [\"fees\", \"expenses\", \"management\", \"compensation\", \"costs\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"(?:base|management|advisory)\\s+fee[:\\s]*(\\d+\\.\\d+%)\",\n",
    "                    r\"(\\d+\\.\\d+%)\\s*(?:of\\s+net\\s+assets|annually|management)\",\n",
    "                    r\"expense\\s+ratio[:\\s]*(\\d+\\.\\d+%)\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"distribution\", \"dividend\", \"yield\", \"return\"],\n",
    "                \"expected_format\": \"percentage_structure\"\n",
    "            },\n",
    "            \"asset_allocation\": {\n",
    "                \"context_keywords\": [\"level 1\", \"level 2\", \"level 3\", \"fair value hierarchy\", \"asset allocation\", \"valuation\"],\n",
    "                \"section_hints\": [\"portfolio\", \"fair value\", \"investments\", \"allocation\", \"valuation\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"Level\\s+([123])[:\\s,]*([0-9]+\\.[0-9]+)%\",\n",
    "                    r\"([0-9]+\\.[0-9]+)%.*?Level\\s+([123])\",\n",
    "                    r\"Level\\s+([123]).*?([0-9]+\\.[0-9]+%).*?portfolio\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"fee\", \"expense\", \"distribution\"],\n",
    "                \"expected_format\": \"level_percentages\"\n",
    "            },\n",
    "            \"suitability\": {\n",
    "                \"context_keywords\": [\"suitability\", \"minimum income\", \"net worth\", \"investor requirements\", \"eligibility\", \"qualified\"],\n",
    "                \"section_hints\": [\"suitability\", \"investor\", \"eligibility\", \"requirements\", \"qualification\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"\\$([0-9,]+)\\s+(?:annual\\s+)?income.*?\\$([0-9,]+)\\s+net\\s+worth\",\n",
    "                    r\"\\$([0-9,]+)\\s+net\\s+worth.*?\\$([0-9,]+)\\s+(?:annual\\s+)?income\",\n",
    "                    r\"(\\$[0-9,]+)\\s+(?:and|or)\\s+(\\$[0-9,]+)\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"fee\", \"expense\", \"distribution\", \"management\"],\n",
    "                \"expected_format\": \"income_networth\"\n",
    "            },\n",
    "            \"liquidity\": {\n",
    "                \"context_keywords\": [\"liquidity\", \"repurchase\", \"quarterly repurchase\", \"not publicly traded\", \"share repurchase\"],\n",
    "                \"section_hints\": [\"liquidity\", \"repurchase\", \"trading\", \"shares\", \"redemption\"],\n",
    "                \"entity_patterns\": [\n",
    "                    r\"not\\s+publicly\\s+traded\",\n",
    "                    r\"quarterly\\s+repurchase\",\n",
    "                    r\"limited\\s+liquidity\",\n",
    "                    r\"repurchase.*?program\",\n",
    "                    r\"(\\d+)%.*?repurchase\"\n",
    "                ],\n",
    "                \"negative_keywords\": [\"fee\", \"distribution\", \"yield\"],\n",
    "                \"expected_format\": \"description\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _extract_pages_with_structure(self) -> List[Dict]:\n",
    "        # Extract text with page numbers and basic structure detection\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        pages_content = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Detect potential section headers \n",
    "            headers = re.findall(r'^[A-Z][A-Z\\s]{10,}$', text, re.MULTILINE)\n",
    "            \n",
    "            # Look for table-like structures\n",
    "            tables = self._detect_tables(text)\n",
    "            \n",
    "            pages_content.append({\n",
    "                \"page_num\": page_num + 1,\n",
    "                \"text\": text,\n",
    "                \"headers\": headers,\n",
    "                \"tables\": tables,\n",
    "                \"has_financial_data\": any(pattern in text.lower() \n",
    "                                        for pattern in [\"$\", \"million\", \"billion\", \"%\", \"rate\", \"fee\"])\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        return pages_content\n",
    "\n",
    "    def _detect_tables(self, text: str) -> List[str]:\n",
    "        #Simple table detection based on patterns\n",
    "        lines = text.split('\\n')\n",
    "        tables = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # Look for lines with multiple dollar amounts or percentages\n",
    "            if len(re.findall(r'\\$[\\d,]+|\\d+\\.\\d+%', line)) >= 2:\n",
    "                # Get surrounding context\n",
    "                start = max(0, i-2)\n",
    "                end = min(len(lines), i+3)\n",
    "                table_context = '\\n'.join(lines[start:end])\n",
    "                tables.append(table_context)\n",
    "        \n",
    "        return tables\n",
    "\n",
    "    def _extract_sentences_with_context(self) -> List[Dict]:\n",
    "        #Extract sentences with page and section context\n",
    "        sentences_with_context = []\n",
    "        \n",
    "        for page_info in self.pages_content:\n",
    "            page_text = page_info[\"text\"]\n",
    "            page_num = page_info[\"page_num\"]\n",
    "            \n",
    "            # Split into sentences\n",
    "            try:\n",
    "                # Process text in smaller chunks to avoid memory issues\n",
    "                chunk_size = 1000000\n",
    "                page_sentences = []\n",
    "                \n",
    "                if len(page_text) > chunk_size:\n",
    "                    for i in range(0, len(page_text), chunk_size):\n",
    "                        chunk = page_text[i:i+chunk_size]\n",
    "                        try:\n",
    "                            doc = self.nlp(chunk)\n",
    "                            chunk_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "                            page_sentences.extend(chunk_sentences)\n",
    "                        except:\n",
    "                            # Fallback to NLTK if spaCy fails\n",
    "                            chunk_sentences = sent_tokenize(chunk)\n",
    "                            page_sentences.extend([s.strip() for s in chunk_sentences if s.strip()])\n",
    "                else:\n",
    "                    try:\n",
    "                        doc = self.nlp(page_text)\n",
    "                        page_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "                    except:\n",
    "                        page_sentences = sent_tokenize(page_text)\n",
    "                        page_sentences = [s.strip() for s in page_sentences if s.strip()]\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing page {page_num}: {e}\")\n",
    "                page_sentences = sent_tokenize(page_text)\n",
    "                page_sentences = [s.strip() for s in page_sentences if s.strip()]\n",
    "            \n",
    "            for sentence in page_sentences:\n",
    "                if len(sentence) > 10:  # Filter out very short sentences\n",
    "                    sentences_with_context.append({\n",
    "                        \"text\": sentence,\n",
    "                        \"page_num\": page_num,\n",
    "                        \"has_financial_data\": page_info[\"has_financial_data\"],\n",
    "                        \"near_headers\": page_info[\"headers\"],\n",
    "                        \"in_table\": any(sentence in table for table in page_info[\"tables\"])\n",
    "                    })\n",
    "        \n",
    "        print(f\"Extracted {len(sentences_with_context)} sentences with context\")\n",
    "        return sentences_with_context\n",
    "\n",
    "    def _get_bert_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        #Get BERT embeddings for a list of texts\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text[:512], return_tensors='pt', truncation=True, padding=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(embedding[0])\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def _enhanced_pattern_search(self, field_name: str, config: Dict) -> List[Tuple[str, float, Dict]]:\n",
    "        # Enhanced pattern-based search with context scoring\n",
    "        candidates = []\n",
    "        \n",
    "        for sent_info in self.sentences_with_context:\n",
    "            sentence = sent_info[\"text\"]\n",
    "            base_score = 0\n",
    "            \n",
    "            # Pattern matching with higher weights\n",
    "            for pattern in config[\"entity_patterns\"]:\n",
    "                matches = re.finditer(pattern, sentence, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    base_score += 5\n",
    "                    \n",
    "            # Context keyword matching\n",
    "            sentence_lower = sentence.lower()\n",
    "            for keyword in config[\"context_keywords\"]:\n",
    "                if keyword in sentence_lower:\n",
    "                    base_score += 2\n",
    "            \n",
    "            # Section relevance bonus\n",
    "            for hint in config[\"section_hints\"]:\n",
    "                for header in sent_info[\"near_headers\"]:\n",
    "                    if hint.lower() in header.lower():\n",
    "                        base_score += 3\n",
    "            \n",
    "            # Financial data context bonus\n",
    "            if sent_info[\"has_financial_data\"] and field_name in [\"total_investments\", \"management_fee\", \"distribution_rate\"]:\n",
    "                base_score += 2\n",
    "            \n",
    "            # Table context bonus for structured data\n",
    "            if sent_info[\"in_table\"] and field_name in [\"asset_allocation\", \"total_investments\"]:\n",
    "                base_score += 3\n",
    "            \n",
    "            # Negative keyword penalty\n",
    "            for neg_kw in config[\"negative_keywords\"]:\n",
    "                if neg_kw in sentence_lower:\n",
    "                    base_score -= 1\n",
    "            \n",
    "            # Special scoring boosts for fund_name and sponsor\n",
    "            if field_name == \"fund_name\":\n",
    "                # Boost for exact fund name matches\n",
    "                if \"blackrock private credit fund\" in sentence_lower:\n",
    "                    base_score += 10\n",
    "                # Boost for early pages (likely to contain fund name)\n",
    "                if sent_info[\"page_num\"] <= 5:\n",
    "                    base_score += 3\n",
    "                    \n",
    "            if field_name == \"sponsor\":\n",
    "                # Boost for BlackRock mentions in advisory context\n",
    "                if \"blackrock\" in sentence_lower and any(word in sentence_lower for word in [\"adviser\", \"advisor\", \"manager\", \"management\"]):\n",
    "                    base_score += 8\n",
    "                # Boost for early pages\n",
    "                if sent_info[\"page_num\"] <= 10:\n",
    "                    base_score += 2\n",
    "            \n",
    "            if base_score > 0:\n",
    "                candidates.append((sentence, base_score, sent_info))\n",
    "        \n",
    "        # Sort by score\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[:20]  # Top 20 candidates\n",
    "\n",
    "    def _semantic_search_enhanced(self, context_keywords: List[str], candidates: List[Tuple]) -> List[Tuple[str, float, Dict]]:\n",
    "        # Enhanced semantic search on pre-filtered candidates\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        query = \" \".join(context_keywords)\n",
    "        query_embedding = self._get_bert_embeddings([query])\n",
    "        \n",
    "        candidate_texts = [cand[0] for cand in candidates]\n",
    "        candidate_embeddings = self._get_bert_embeddings(candidate_texts)\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
    "        \n",
    "        # Combine pattern score with semantic similarity\n",
    "        enhanced_candidates = []\n",
    "        for i, (sentence, pattern_score, context) in enumerate(candidates):\n",
    "            combined_score = pattern_score * 0.7 + similarities[i] * 10 * 0.3\n",
    "            enhanced_candidates.append((sentence, combined_score, context))\n",
    "        \n",
    "        enhanced_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return enhanced_candidates[:10]\n",
    "\n",
    "    def _extract_specific_value(self, sentence: str, field_name: str, config: Dict) -> Optional[str]:\n",
    "        # Extract specific values using targeted patterns\n",
    "        \n",
    "        if field_name == \"fund_name\":\n",
    "            # Enhanced fund name patterns with priority order\n",
    "            patterns = [\n",
    "                # Exact match patterns (highest priority)\n",
    "                r\"(BlackRock\\s+Private\\s+Credit\\s+Fund)(?!\\s+Advisors)\",\n",
    "                r\"(?:The\\s+)?(BlackRock\\s+Private\\s+Credit\\s+Fund)\",\n",
    "                \n",
    "                # Partial match patterns\n",
    "                r\"Fund\\s+Name[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                r\"Registrant[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                r\"Investment\\s+Company[:\\s]+([A-Za-z\\s,&]*BlackRock[A-Za-z\\s,&]*Fund)\",\n",
    "                \n",
    "                # Generic BlackRock fund patterns\n",
    "                r\"(BlackRock\\s+[A-Za-z\\s&]*Credit\\s+Fund)\",\n",
    "                r\"BlackRock\\s+([A-Za-z\\s&]+(?:Fund|Trust|Income|Credit))\",\n",
    "                r\"([A-Za-z\\s&]*Private\\s+Credit\\s+Fund)\"\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    name = match.group(1).strip()\n",
    "                    name = re.sub(r'\\s+', ' ', name)\n",
    "                    name = re.sub(r'^(the\\s+)', '', name, flags=re.IGNORECASE)\n",
    "                    # Prefer exact matches\n",
    "                    if \"BlackRock Private Credit Fund\" in name:\n",
    "                        return \"BlackRock Private Credit Fund\"\n",
    "                    return name\n",
    "        \n",
    "        elif field_name == \"sponsor\":\n",
    "            patterns = [\n",
    "                r\"Investment\\s+Adviser?[:\\s]+(BlackRock)(?:\\s+Inc\\.?|\\s+Fund\\s+Advisors?|\\s+Advisors?|$|\\s+serves|\\s+acts)\",\n",
    "                r\"Sponsor[:\\s]+(BlackRock)(?:\\s+Inc\\.?|$|\\s+serves|\\s+acts)\",\n",
    "                r\"(BlackRock)(?:\\s+Inc\\.?)?\\s+(?:serves\\s+as|acts\\s+as|is\\s+the).*?(?:adviser?|advisor|manager)\",\n",
    "                \n",
    "                r\"Investment\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Advisors?|Management|Fund\\s+Advisors?))\",\n",
    "                r\"Sponsor[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Fund\\s+Advisors?))\",\n",
    "                r\"managed\\s+by\\s+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*(?:Inc|LLC|Advisors?))\",\n",
    "                r\"Fund\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]*BlackRock[A-Za-z\\s,&.]*)\",\n",
    "                \n",
    "                r\"Investment\\s+Adviser?[:\\s]+([A-Za-z\\s,&.]+(?:Inc|LLC|LP|Advisers?))\"\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    sponsor = match.group(1).strip()\n",
    "                    # Clean up extra whitespace and trailing punctuation\n",
    "                    sponsor = re.sub(r'\\s+', ' ', sponsor)\n",
    "                    sponsor = re.sub(r'[.,;]+$', '', sponsor)\n",
    "                    \n",
    "                    # Prefer simple \"BlackRock\" if found\n",
    "                    if sponsor.lower() == \"blackrock\" or sponsor.lower() == \"blackrock inc\":\n",
    "                        return \"BlackRock\"\n",
    "                    elif \"blackrock\" in sponsor.lower():\n",
    "                        return sponsor\n",
    "                    else:\n",
    "                        return sponsor\n",
    "        \n",
    "        elif field_name == \"inception_year\":\n",
    "            # Look for years near inception keywords\n",
    "            patterns = [\n",
    "                r\"(?:inception|commenced|launched|formed).{0,50}(20\\d{2})\",\n",
    "                r\"(20\\d{2}).{0,50}(?:inception|commenced|launched)\",\n",
    "                r\"Fund\\s+inception[:\\s]+(20\\d{2})\",\n",
    "                r\"since\\s+(20\\d{2})\",\n",
    "                r\"established\\s+in\\s+(20\\d{2})\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "        \n",
    "        elif field_name == \"total_investments\":\n",
    "            # Look for dollar amounts with investment context\n",
    "            patterns = [\n",
    "                r\"[Tt]otal\\s+investments[:\\s]*\\$([0-9,]+(?:\\.[0-9]+)?)\\s*(?:million|billion)?\",\n",
    "                r\"\\$([0-9,]+(?:\\.[0-9]+)?)\\s*(?:million|billion)?.{0,30}(?:total\\s+investments|fair\\s+value)\",\n",
    "                r\"(?:Total|Net)\\s+(?:investments|assets)[:\\s]*\\$([0-9,]+(?:\\.[0-9]+)?)\\s*(?:million|billion)?\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    amount = match.group(1)\n",
    "                    # Check for million/billion modifier\n",
    "                    if re.search(r'million', sentence, re.IGNORECASE):\n",
    "                        return f\"${amount} million\"\n",
    "                    elif re.search(r'billion', sentence, re.IGNORECASE):\n",
    "                        return f\"${amount} billion\"\n",
    "                    return f\"${amount}\"\n",
    "        \n",
    "        elif field_name == \"management_fee\":\n",
    "            # Look for fee structure \n",
    "            fee_context = sentence.lower()\n",
    "            if any(word in fee_context for word in ['base fee', 'management fee', 'advisory fee']):\n",
    "                # Look for clean percentage patterns\n",
    "                fees = re.findall(r\"(\\d+\\.\\d+%)\", sentence)\n",
    "                # Filter out obviously wrong percentages (like >50%)\n",
    "                clean_fees = [fee for fee in fees if float(fee.replace('%', '')) <= 10.0]\n",
    "                if clean_fees:\n",
    "                    return \" + \".join(clean_fees) if len(clean_fees) > 1 else clean_fees[0]\n",
    "        \n",
    "        elif field_name == \"distribution_rate\":\n",
    "            # Look for distribution rates or monthly amounts\n",
    "            patterns = [\n",
    "                r\"(\\d+\\.\\d+%)\\s*(?:annualized|effective|target|distribution)\",\n",
    "                r\"(?:annualized|effective|target)\\s+(?:distribution\\s+rate|yield)[:\\s]*(\\d+\\.\\d+%)\",\n",
    "                r\"\\$(\\d+\\.\\d+)\\s*(?:per\\s+share|\\/share).*?(?:monthly|quarterly|annual)\",\n",
    "                r\"target.*?(\\d+\\.\\d+%)\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "                if match:\n",
    "                    value = match.group(1)\n",
    "                    if '$' in value:\n",
    "                        return f\"${value}/share\"\n",
    "                    return value\n",
    "        \n",
    "        elif field_name == \"asset_allocation\":\n",
    "            # Look for level percentages - extract all levels from sentence\n",
    "            levels_found = {}\n",
    "            patterns = [\n",
    "                r\"Level\\s+([123])[:\\s,]*([0-9]+\\.[0-9]+)%\",\n",
    "                r\"([0-9]+\\.[0-9]+)%.*?Level\\s+([123])\"\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, sentence, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    if len(match) == 2:\n",
    "                        if pattern.startswith(\"Level\"):\n",
    "                            level, pct = match\n",
    "                        else:\n",
    "                            pct, level = match\n",
    "                        levels_found[level] = f\"{pct}%\"\n",
    "            \n",
    "            if levels_found:\n",
    "                result = []\n",
    "                for level in ['1', '2', '3']:\n",
    "                    if level in levels_found:\n",
    "                        result.append(f\"Level {level}: {levels_found[level]}\")\n",
    "                return \"; \".join(result)\n",
    "        \n",
    "        elif field_name == \"suitability\":\n",
    "            # Look for income/net worth requirements\n",
    "            amounts = re.findall(r\"\\$([0-9,]+)\", sentence)\n",
    "            if len(amounts) >= 2:\n",
    "                return f\"${amounts[0]} income and ${amounts[1]} net worth\"\n",
    "            elif len(amounts) == 1:\n",
    "                return f\"${amounts[0]}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_field_enhanced(self, field_name: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Enhanced field extraction combining multiple techniques\"\"\"\n",
    "        assumptions = []\n",
    "        config = self.field_contexts[field_name]\n",
    "        \n",
    "        # Step 1: Pattern-based candidate filtering\n",
    "        pattern_candidates = self._enhanced_pattern_search(field_name, config)\n",
    "        \n",
    "        if not pattern_candidates:\n",
    "            assumptions.append(f\"{field_name} - no pattern matches found\")\n",
    "            return \"\", assumptions\n",
    "        \n",
    "        # Step 2: Semantic refinement\n",
    "        semantic_candidates = self._semantic_search_enhanced(config[\"context_keywords\"], pattern_candidates)\n",
    "        \n",
    "        # Step 3: Extract specific values\n",
    "        for sentence, score, context in semantic_candidates:\n",
    "            extracted_value = self._extract_specific_value(sentence, field_name, config)\n",
    "            \n",
    "            if extracted_value:\n",
    "                confidence = \"high\" if score > 8 else \"medium\" if score > 5 else \"low\"\n",
    "                if confidence != \"high\":\n",
    "                    assumptions.append(f\"{field_name} extracted with {confidence} confidence (score: {score:.1f})\")\n",
    "                \n",
    "                return extracted_value, assumptions\n",
    "        \n",
    "        # Step 4: Fallback to best context\n",
    "        if semantic_candidates:\n",
    "            best_sentence, best_score, best_context = semantic_candidates[0]\n",
    "            assumptions.append(f\"{field_name} - using best context match (score: {best_score:.1f})\")\n",
    "            return best_sentence[:200] + \"...\" if len(best_sentence) > 200 else best_sentence, assumptions\n",
    "        \n",
    "        assumptions.append(f\"{field_name} - no suitable candidates found\")\n",
    "        return \"\", assumptions\n",
    "\n",
    "    def extract(self) -> FundInfo:\n",
    "        \"\"\"Extract fund information using enhanced hybrid approach\"\"\"\n",
    "        info = FundInfo()\n",
    "        all_assumptions = []\n",
    "        \n",
    "        field_mapping = {\n",
    "            \"total_investments\": \"total_investments_fair_value\",\n",
    "            \"suitability\": \"suitability_requirements\", \n",
    "            \"distribution_rate\": \"annualized_distribution_rate\"\n",
    "        }\n",
    "        \n",
    "        for field_name in tqdm(self.field_contexts.keys(), desc=\"Extracting fields\"):\n",
    "            field_attr = field_mapping.get(field_name, field_name)\n",
    "            \n",
    "            value, assumptions = self._extract_field_enhanced(field_name)\n",
    "            setattr(info, field_attr, value)\n",
    "            all_assumptions.extend(assumptions)\n",
    "        \n",
    "        info.assumptions_made = all_assumptions\n",
    "        return info\n",
    "\n",
    "    def to_json(self, info: FundInfo) -> str:\n",
    "        return json.dumps(asdict(info), indent=2, ensure_ascii=False)\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/nikhithasivaprakasam/Downloads/Blackrock Prospectus.pdf\"  # Update this path\n",
    "    \n",
    "    try:\n",
    "        parser = EnhancedFundParser(pdf_path)\n",
    "        fund_info = parser.extract()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED EXTRACTION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(parser.to_json(fund_info))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4cabb-35f3-461d-ab59-b8b413f864f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
